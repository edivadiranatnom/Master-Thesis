{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, logging\n",
    "import tqdm\n",
    "from transformers import BertConfig, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(example_file, tokenizer, shuffle=True):\n",
    "    f = json.load(open(example_file, 'r'))\n",
    "    examples = [ex for ex in f]\n",
    "    features = []\n",
    "    for example in tqdm.tqdm(examples):\n",
    "        print(example)\n",
    "        if isinstance(example[\"src\"], list):\n",
    "            source_tokens = example[\"src\"]\n",
    "            target_tokens = example[\"tgt\"]\n",
    "        else:\n",
    "            source_tokens = tokenizer.tokenize(text=example[\"src\"])\n",
    "            target_tokens = tokenizer.tokenize(text=example[\"tgt\"])\n",
    "        features.append({\n",
    "                \"source_ids\": tokenizer.convert_tokens_to_ids(source_tokens),\n",
    "                \"target_ids\": tokenizer.convert_tokens_to_ids(target_tokens),\n",
    "            })\n",
    "    if shuffle:\n",
    "        random.shuffle(features)\n",
    "    return features\n",
    "\n",
    "def get_model_and_tokenizer():\n",
    "    config_class, tokenizer_class = (BertConfig, BertTokenizer)\n",
    "    model_config = config_class.from_pretrained('unilm', cache_dir='./cache')\n",
    "    config = BertForSeq2SeqConfig.from_exist_config(\n",
    "        config=model_config, label_smoothing=0.1,\n",
    "        max_position_embeddings=40)\n",
    "\n",
    "    logger.info(\"Model config for seq2seq: %s\", str(config))\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "        do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "\n",
    "    model = BertForSequenceToSequence.from_pretrained(\n",
    "        args.model_name_or_path, config=config, model_type=args.model_type,\n",
    "        reuse_position_embedding=True,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{'src': 'giraffes zoo enclosure', 'tgt': 'Two giraffes standing in an enclosure at the zoo.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tokenize() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-507bf1f22018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../wsd_json/raw_src_tgt.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-03110935540a>\u001b[0m in \u001b[0;36mload_examples\u001b[0;34m(example_file, tokenizer, shuffle)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tgt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0msource_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tgt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         features.append({\n",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer()\n",
    "features = load_examples('../wsd_json/raw_src_tgt.json', BertTokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
